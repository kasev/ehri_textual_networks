{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PREREQUISITIES\n",
    "\n",
    "### these are basic and should go easy\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string as str_p\n",
    "import collections\n",
    "import regex as re\n",
    "\n",
    "### NLTK  - natural language processing \n",
    "###this requires installation\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "### Beautiful Soup and Urllib\n",
    "### for scrapping of web data and parsing xml files\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### for network analysis\n",
    "import networkx as nx\n",
    "\n",
    "### for visualization\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.io as pio\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse the xml\n",
    "with open(\"data/EHRI-ET-JMP010_EN.xml\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Auschwitz          II., Birkenau', 'Oranienburg,', 'Sachsenhausen', 'Kauffering', 'Dachau', 'Flossenbürg', 'Auschwitz          I', 'Auschwitz I', 'Buna']\n"
     ]
    }
   ],
   "source": [
    "### find/identify camps\n",
    "list_of_camps = []\n",
    "for link in soup.find_all(\"placename\", type=\"camp\"):\n",
    "    camp = [link.get_text(), link.get(\"ref\")]\n",
    "    if camp[1] not in [element[1] for element in list_of_camps]:\n",
    "        list_of_camps.append(camp)\n",
    "print([element[0].replace(\"\\n\", \"\") for element in list_of_camps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find/identify geonames\n",
    "def replacer_place(string):\n",
    "    return re.split(r'\\.html', re.split(r'\\.org\\/', string)[1])[0].replace(\"/\", \"_\")\n",
    "place_names = []\n",
    "for place in soup.find_all(\"placename\"):\n",
    "    if \"geonames\" in str(place.get):\n",
    "        place_names.append([place.get_text(),\"geoname_\" + replacer_place(place.get(\"ref\"))])\n",
    "    if \"ehri\" in str(place.get):\n",
    "        place_names.append([place.get_text(), \"ehri_camp_\" + place.get(\"ref\").partition(\"ehri_camps-\")[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Zeilsheim', 'geoname_2804979_zeilsheim'],\n",
       " ['Zeisheim', 'geoname_2804979_zeilsheim'],\n",
       " ['Frankfurt', 'geoname_2925533_frankfurt-am-main'],\n",
       " ['Auschwitz\\n          II., Birkenau', 'ehri_camp_2'],\n",
       " ['Plonsk', 'geoname_3088802_plonsko'],\n",
       " ['Auschwitz\\n          II. Birkenau', 'ehri_camp_2'],\n",
       " ['Oranienburg,', 'ehri_camp_847'],\n",
       " ['Sachsenhausen', 'ehri_camp_803'],\n",
       " ['Kauffering', 'ehri_camp_225'],\n",
       " ['Dachau', 'ehri_camp_177']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### look at the geonames tuples\n",
    "place_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Michal Frankl', 'name_Michal_Frankl'],\n",
       " ['Magdalena Sedlická', 'name_Magdalena_Sedlická'],\n",
       " ['Wolfgang Schellenbacher', 'name_Wolfgang_Schellenbacher'],\n",
       " ['Hermann Przewoznik', 'name_Hermann_Przewoznik'],\n",
       " ['Šlomo Fuchs', 'name_Šlomo_Fuchs'],\n",
       " ['Chaim Schwarzwald', 'name_Chaim_Schwarzwald'],\n",
       " ['Hermann Przewozník', 'name_Hermann_Przewozník'],\n",
       " ['Šlomo\\n          Fuchs', 'name_Šlomo_Fuchs'],\n",
       " ['Chaim Schwarzwald', 'name_Chaim_Schwarzwald'],\n",
       " ['Fuchs', 'name_Fuchs']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### find personal names\n",
    "def replacer_pers(url):\n",
    "    return url.partition(\"ehri_pers-\")[2]\n",
    "persnames = []\n",
    "for persname in soup.find_all(\"persname\"):\n",
    "    if persname.get(\"ref\") != None:\n",
    "        persnames.append([persname.get_text(), \"ehri_name_\" + replacer_pers(persname.get(\"ref\"))])\n",
    "    else:\n",
    "        persname_changed =  \"name_\" + persname.get_text().replace(\" \", \"_\")\n",
    "        if \"__\" in persname_changed:\n",
    "            persname_changed = re.sub(r'__+', '_', persname_changed)\n",
    "        persnames.append([persname.get_text(), persname_changed.replace(\"\\n\", \"\")])\n",
    "persnames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract raw text from the \"soup\" as a list of paragraphs\n",
    "raw_text = []\n",
    "all_p = soup.body.find_all(\"p\")\n",
    "for p in all_p:\n",
    "    raw_text.append(p.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We worked in the\\xa0Sauna – Birkenau\\n        disinfection chamber – from January 1943 to January 1945, i.e. until the evacuation, I, Herman Przewoznik, as a scribe, I,\\n          Šlomo Fuchs, as a worker '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### make string from the list of paragraphs\n",
    "raw_text_str = \" \".join(raw_text[5:])\n",
    "raw_text_str[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We worked in the\\xa0Sauna – ehri_camp_2\\n        disinfection chamber – from January 1943 to January 1945, i.e. until the evacuation, I, name_Herman_Przewoznik, as a scribe, I,\\n          name_Šlomo_Fuchs,'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### replace the raw text versions of place names and personal names by the preprocessed versions\n",
    "for place in place_names:\n",
    "    raw_text_str = raw_text_str.replace(\" \" + place[0], \" \" + place[1], 1)\n",
    "for person in persnames:\n",
    "    raw_text_str = raw_text_str.replace(\" \" + person[0], \" \" + person[1], 1)\n",
    "raw_text_str[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove newlines characters\n",
    "cleaned_text_1  = \" \".join(raw_text_str.splitlines())\n",
    "cleaned_text_2 = cleaned_text_1.replace(\"\\xa0\", \" \")\n",
    "if \"  \" in cleaned_text_2:\n",
    "    cleaned_text_3 = re.sub(r'\\s\\s+', ' ', cleaned_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'worked', 'in', 'the', 'Sauna', '–', 'ehri_camp_2', 'disinfection', 'chamber', '–', 'from', 'January', '1943', 'to', 'January', '1945', 'ie', 'until', 'the', 'evacuation']\n"
     ]
    }
   ],
   "source": [
    "### remove punctuation\n",
    "for character in str_p.punctuation:\n",
    "    if character != \"_\":\n",
    "        cleaned_text_3 = cleaned_text_3.replace(character, \"\")\n",
    "cleaned_text_list = [word for word in cleaned_text_3.split()]\n",
    "print(cleaned_text_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization and POS-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lem(word):\n",
    "    wordnet_lemmatizer.lemmatize(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'work', 'in', 'the', 'sauna', '–', 'ehri_camp_2', 'disinfection', 'chamber', '–', 'from', 'january', '1943', 'to', 'january', '1945', 'ie', 'until', 'the', 'evacuation', 'i', 'name_herman_przewoznik', 'a', 'a', 'scribe', 'i', 'name_šlomo_fuchs', 'a', 'a', 'worker', 'on', 'the', 'socalled', 'unclean', 'side', 'and', 'i', 'name_chaim_schwarzwald', 'a', 'a', 'worker', 'on', 'the', 'ramp', 'when', 'the', 'transport', 'arrive', 'the', 'people', 'be', 'pull', 'out', 'of', 'the', 'carriage', 'their', 'thing', 'be', 'take', 'away', 'and', 'then', 'they', 'be', 'turn', 'over', '–', 'men', 'and', 'woman', 'separately', '–', 'for', 'selection', 'by', 'the', 's', 'doctor', 'mengele', 'ehri_name_001165', 'name_horstmann', 'name_könig', 'and', 'others', 'who', 'have', 'the', 's', 'rank', 'of', 'ober', 'or', 'hauptsturmführer', 'men', 'always', 'stand', 'on', 'the', 'right']\n"
     ]
    }
   ],
   "source": [
    "### lemmatize as verbs\n",
    "lemmatized_document = [wordnet_lemmatizer.lemmatize(word.lower(), pos=\"v\") for word in cleaned_text_list]\n",
    "### lemmatize as nouns\n",
    "lemmatized_document = [wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in lemmatized_document]\n",
    "print(lemmatized_document[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 'PRP'), ('work', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('sauna', 'NN'), ('–', 'NNP'), ('ehri_camp_2', 'VBZ'), ('disinfection', 'NN'), ('chamber', 'NN'), ('–', 'NN'), ('from', 'IN'), ('january', 'JJ'), ('1943', 'CD'), ('to', 'TO'), ('january', 'JJ'), ('1945', 'CD'), ('ie', 'NN'), ('until', 'IN'), ('the', 'DT'), ('evacuation', 'NN'), ('i', 'NN'), ('name_herman_przewoznik', 'VBP'), ('a', 'DT'), ('a', 'DT'), ('scribe', 'NN'), ('i', 'NN'), ('name_šlomo_fuchs', 'VBP'), ('a', 'DT'), ('a', 'DT'), ('worker', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "### generate POS-tags\n",
    "postagged_document = nltk.pos_tag(lemmatized_document)\n",
    "print(postagged_document[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JJS', 'CD', 'RBR', 'WRB', 'NNP', 'MD', 'RBS', 'VB', 'WP', 'TO', 'VBZ', 'FW', 'NNS', 'VBN', 'WDT', 'IN', '$', 'JJ', 'RB', 'DT', 'PDT', 'JJR', 'VBG', 'PRP', 'EX', 'VBD', 'NN', 'RP', 'CC', 'VBP', 'PRP$'}\n"
     ]
    }
   ],
   "source": [
    "### check unique values of TAGS\n",
    "print(set([pos_tagged[1] for pos_tagged in postagged_document]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to identify our geonames and place names\n",
    "exceptions = []\n",
    "for element in (set(lemmatized_document)):\n",
    "    if (\"_\") in element:\n",
    "        exceptions.append(element)\n",
    "    if element == \"ss\":\n",
    "        exceptions.append(element.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 'PRP'), ('work', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('sauna', 'NN'), ('–', 'NNP'), ('ehri_camp_2', 'VBZ'), ('disinfection', 'NN'), ('chamber', 'NN'), ('–', 'NN'), ('from', 'IN'), ('january', 'JJ'), ('1943', 'CD'), ('to', 'TO'), ('january', 'JJ'), ('1945', 'CD'), ('ie', 'NN'), ('until', 'IN'), ('the', 'DT'), ('evacuation', 'NN'), ('i', 'NN'), ('name_herman_przewoznik', 'VBP'), ('a', 'DT'), ('a', 'DT'), ('scribe', 'NN'), ('i', 'NN'), ('name_šlomo_fuchs', 'VBP'), ('a', 'DT'), ('a', 'DT'), ('worker', 'NN'), ('on', 'IN'), ('the', 'DT'), ('socalled', 'JJ'), ('unclean', 'JJ'), ('side', 'NN'), ('and', 'CC'), ('i', 'NN'), ('name_chaim_schwarzwald', 'VBP'), ('a', 'DT'), ('a', 'DT'), ('worker', 'NN'), ('on', 'IN'), ('the', 'DT'), ('ramp', 'NN'), ('when', 'WRB'), ('the', 'DT'), ('transport', 'NN'), ('arrive', 'VBD'), ('the', 'DT'), ('people', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(postagged_document[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'sauna', 'ehri_camp_2', 'disinfection', 'chamber', 'january', 'january', 'evacuation', 'name_herman_przewoznik', 'scribe', 'name_šlomo_fuchs', 'worker', 'socalled', 'unclean', 'side', 'name_chaim_schwarzwald', 'worker', 'ramp', 'transport', 'arrive', 'people', 'pull', 'carriage', 'thing', 'take', 'turn', 'men', 'woman', 'selection', 's']\n"
     ]
    }
   ],
   "source": [
    "### to subselect only nouns, verbs and adjectives\n",
    "document_filtered = []\n",
    "for pos_tuple in postagged_document:\n",
    "    if (pos_tuple[1] in [\"NN\", \"NNP\", \"NNS\", \"JJ\", \"JJR\", \"JJS\", 'VB', 'VBD', 'VBG','VBN', 'VBP','VBZ']) or (pos_tuple[0] in exceptions):\n",
    "        ### ignore some words\n",
    "        if pos_tuple[0] not in ['–', \"ie\", \"i\", \"be\"]:\n",
    "            document_filtered.append(pos_tuple[0])    \n",
    "print(document_filtered[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1919"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network formation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### import our own module for network buiding\n",
    "import network_functions as nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate the network using external module\n",
    "document_network = nf.network_formation_text(document_filtered, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check the number of edges\n",
    "len(document_network.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kasev/106.embed\" height=\"700px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### drawing the network using exteral module\n",
    "nf.draw_2d_network(document_network, \"EHRI example 2D network (verbs, nouns, and adjectives)\", \"online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kasev/108.embed\" height=\"700px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### drawing the network using exteral module\n",
    "nf.draw_3d_network(document_network, \"EHRI example 3D network (verbs, nouns, and adjectives)\", \"online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
